{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:4px; border-color:coral\"></hr>\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "<hr style=\"border-width:4px; border-color:coral\"></hr>\n",
    "\n",
    "Central to numerical linear algebra is the \"singular value decomposition\".  This is one of the most general of the matrix decompositions, in that it can be applied to any matrix (singluar, non-singular, square, or non-square).  \n",
    "\n",
    "But before continuing, we do a brief review of the eigenvalue decomposition to see where it can fail to be completely general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review : Eigenvalue decomposition\n",
    "\n",
    "<hr style=\"border-width:3px; border-color:black\"></hr>\n",
    "\n",
    "Eigenvalue/eigenvector pairs $(\\lambda, \\mathbf v)$ of a square matrix $A \\in \\mathbb R^{m \\times m}$ satisfy\n",
    "\n",
    "\\begin{equation}\n",
    "A\\mathbf v = \\lambda \\mathbf v\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda \\in \\mathbb C$, and $\\mathbf v \\in \\mathbb C^m$.  Because eigenvalues are the roots of the *characteristic polynomial* $p(\\lambda) = \\det(A - \\lambda I)$, we will always have $m$ eigenvalues. Eigenvalues can have multiplicities greater than 1.  \n",
    " \n",
    "\n",
    "The **eigenvectors** associated  with each distinct eigenvalue satisfy\n",
    "\n",
    "\\begin{equation}\n",
    "(A - \\lambda I)\\mathbf v = 0.\n",
    "\\end{equation}\n",
    "\n",
    "The subspace spanned by the vectors in the nullspace of $A - \\lambda I$ form the *eigenspace* associated with $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Algebraic multipicity and geometric multiplicity\n",
    "\n",
    "* The *algebraic multiplicity* of an eigenvalue $\\lambda$ refers to multiplicity of $\\lambda$ as a root of the characteristic polynomial $p(\\lambda)$. \n",
    "\n",
    "\n",
    "* The *geometric multiplicity* of an eigenvalue $\\lambda$ is the dimension of the eigenspace associated with $\\lambda$. \n",
    "\n",
    "\n",
    "In general, the geometric multiplcity is less than or equal to the algebraic multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Defective eigenvalues\n",
    "\n",
    "An eigenvalue of said to be *defective* if its geometric multiplicity is less than its algebraic multiplicity.  A matrix with a defective eigenvalue is a *defective matrix*. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "#### Example : Defective matrix\n",
    "\n",
    "The matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "is defective. We can see immediately that it has the single eigenvalue $\\lambda=0$ with algebraic multiplicity 2.  But the nullspace of $A - \\lambda I = A$ has only the single vector $\\mathbf v = (1,0)^T$, so the geometric multiplicity of $\\lambda$ is 1. \n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Eigenvectors associated with distinct eigenvalues\n",
    "\n",
    "Eigenvectors associated with distinct eigenvalues are *linearly independent*. \n",
    "\n",
    "#### Proof\n",
    "\n",
    "Prove for two eigenvectors associated with distinct eigenvalues.\n",
    "\n",
    "Start : Assume $A \\mathbf x =\\lambda \\mathbf x$, and $A \\mathbf y =\\mu \\mathbf y$, where $\\lambda \\ne \\mu$.  Assume $\\mathbf x = \\beta \\mathbf y$ for some $\\beta \\ne 0$.  We have \n",
    "$A\\mathbf x = A (\\beta \\mathbf y) = \\beta A\\mathbf y = \\beta \\mu \\mathbf y$.  But we also have \n",
    "$A \\mathbf x = \\lambda \\mathbf x = \\beta \\lambda \\mathbf y$.  Combining these two equalities, we have $\\beta \\lambda \\mathbf y =\n",
    "\\beta \\mu \\mathbf y$, from which we conclude that $\\lambda = \\mu$, which is a contradiction. So our assumption that $\\mathbf x = \\beta \\mathbf y$ is wrong, and so $\\mathbf x$ and $\\mathbf y$ are linearly independent. \n",
    "\n",
    "**Note:** The contrapositive of this statement is not true!  \n",
    "\n",
    "**Homework problem:** prove that eigenvectors associated with distinct eigenvalues of a hermitian matrix are orthogonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagonalizable matrix\n",
    "\n",
    "A matrix $A \\in \\mathbb C^{m \\times m}$ is *diagonalizable* if there exists a matrix $R \\in \\mathbb C^{m \\times m}$ so that \n",
    "\n",
    "\\begin{equation}\n",
    "R^{-1}AR = \\Lambda = \\mbox{diag}\\left(\\lambda_1, \\lambda_2, \\dots, \\lambda_m\\right)\n",
    "\\end{equation}\n",
    "\n",
    "A matrix $A \\in \\mathbb C^{m \\times m}$ is diagonalizable if and only if $A$ has no defective eigenvalues.  (Golub and Van Loan, Corollary 7.1.8).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "#### Example : Distinct eigenvalues\n",
    "\n",
    "An $m \\times m$ matrix with $m$ distinct eigenvalues is diagonalizable.  \n",
    "\n",
    "The contrapostive does not follow! (Think of the identity matrix).  $ A  = I$; $A = I^{-1} A I$ (:-))\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigen-decomposition\n",
    "\n",
    "A diagonalizable matrix $A \\in \\mathbb C^{m \\times m}$ can be *decomposed* as \n",
    "\n",
    "\\begin{equation}\n",
    "A = R \\Lambda R^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "where the columns of $R$ are the eigenvectors of $A$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "#### Example : Spectral Theorem\n",
    "\n",
    "\n",
    "All real symmetric matrices $A \\in \\mathbb R^{m \\times m}$ can be decomposed as \n",
    "\n",
    "\\begin{equation}\n",
    "A = Q^T \\Lambda Q\n",
    "\\end{equation}\n",
    "\n",
    "where both $Q$ and $\\Lambda$ are real matrices in $R^{m \\times m}$.  The matrix $Q$ is a orthogonal matrix whose columns are the normalized eigenvectors of $A$, and $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$.\n",
    "\n",
    "The Spectral Thoeorem states that not only is a symmetric matrix diagonalizable, it can be *unitarily* diagonalized. Its eigenvectors can be chosen to form an orthonormal set.   \n",
    "\n",
    "#### Proof\n",
    "\n",
    "It is a homework exercise to show the eigenvalues of a symmetric matrix are real and that eigenvectors associated with *distinct* eigenvalues of are orthogonal.  It is more challenging to prove that there are no defective eigenvalues and that the eigenvectors associated with the same eigenvalue are orthogonal.  \n",
    "\n",
    "**Outline of the proof:**  The proof relies on the *Real Schur Decomposition*.  Every square matrix can be written in Real Schur form as $A = Q T Q^T$, where $T$ is a block triangular matrix with $1 \\times 1$ or $2 \\times 2$ blocks and $Q$ is an orthogonal matrix.  Since $A$ is symmetric, $T$ can be written as the *direct sum* of $1 \\times 1$ or symmetric $2 \\times 2$ blocks, with $2 \\times 2$ blocks corresponding to complex eigenvalues of $A$. But since a symmetric $2 \\times 2$ block has real eigenvalues, $T$ has only $1 \\times 1$ blocks, and so $A$ can be decomposed as \n",
    "\n",
    "\\begin{equation}\n",
    "QAQ^T = \\mbox{diag}(\\lambda_1, \\lambda_2, \\dots \\lambda_m)\n",
    "\\end{equation}\n",
    "\n",
    "For a complete proof, see Golub and Van Loan, 7.4.1 (Real Schur Decomposition) and 8.1.1 (Eigenvalues of symmetric matrices). \n",
    "\n",
    "**Note:** An *orthogonal* matrix $Q$ is a real matrix satisfying $Q^T Q = I$.   The term *unitary* is reserved for matrices with general  complex entries. \n",
    "\n",
    "**Note:** A *direct sum* $A \\bigoplus B \\bigoplus + \\dots$  is a block diagonal matrix whose diagonal entries are the matrices $A$,  $B$, $\\dots$. See [direct sum of matrices](https://en.wikipedia.org/wiki/Direct_sum#Direct_sum_of_matrices).\n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:4px; border-color:coral\"></hr>\n",
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "<hr style=\"border-width:4px; border-color:coral\"></hr>\n",
    "\n",
    "From the above, it is clear that not every matrix can be decomposed as $R\\Lambda R^{-1}$.  First, we have restricted the eigen-decompostion to only square matrices.  Second, the eigen-decomposition is only available for diagonalizable, square matrices.  And third, the eigenvectors are in general not orthogonal. \n",
    "\n",
    "A more general matrix decomposition that solves the above is the *Singular Value Decompostion*. \n",
    "\n",
    "\n",
    "### Singular values\n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "The *singular values* of a matrix $A \\in \\mathbb C^{m \\times n}$ with rank $r$ are values $\\sigma_i$ satisfying\n",
    "\n",
    "\\begin{equation}\n",
    "A\\mathbf v_i = \\sigma_i \\mathbf u_i, \\qquad i = 1,2,\\dots r\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf u_i$ and $\\mathbf v_i$ are left and right singular vectors, respectively. \n",
    "\n",
    "A few key properties of the singular values/vectors : \n",
    "\n",
    "* The singular values $\\sigma \\in \\mathbb R$ are non-negative, \n",
    "\n",
    "* The right singular vectors $\\mathbf v \\in \\mathbf C^{n}$ are in the *row space* of $A$, and \n",
    "\n",
    "* The left singular vectors $\\mathbf u \\in \\mathbf C^{m}$ are in the *column space* of $A$.   \n",
    "\n",
    "The singular values need not be distinct.  \n",
    "\n",
    "By convention, singular values are numbered in descending order so that $\\sigma_1 \\ge \\sigma_2 \\ge 2 \\dots \\sigma_r > 0$, with $\\sigma_{r+1} = \\sigma_{r+2} = \\dots = \\sigma_{n} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "If we assume that the singular vectors are normalized, we can collect the singular values and singular vectors into semi-unitary and diagonal matrices $U$, $V$ and $\\Sigma$. Then $A$ can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "A = U\\Sigma V^*\n",
    "\\end{equation}\n",
    "\n",
    "where $U \\in \\mathbb C^{m \\times r}$ and $V \\in \\mathbf C^{n \\times r}$ have orthonormal columns, and $\\Sigma \\in \\mathbb R^{r \\times r}$ is a diagonal matrix.  \n",
    "\n",
    "The above decompostion is referred to as a \"reduced\" decomposition, since we might have $r < \\min(m,n)$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "#### Example : SVD of an outer product matrix\n",
    "\n",
    "\n",
    "What is the singular value decomposition of the matrix formed as an outer product?  \n",
    "\n",
    "#### Solution\n",
    "\n",
    "Suppose that $A = \\mathbf u \\mathbf v^*$, where $\\mathbf u \\in \\mathbb C^m$ and $\\mathbf v \\in \\mathbb C^n$. In this case, $A$ is rank deficient, and we have only one non-zero singular value.  The decomposition is \n",
    "\n",
    "\\begin{equation}\n",
    "U = \\frac{\\mathbf u}{\\Vert \\mathbf u\\Vert},  \\qquad V = \\frac{\\mathbf v}{\\Vert \\mathbf v\\Vert}, \\qquad \\Sigma = \\left[\\sigma_1\\right].\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma_1 = \\Vert \\mathbf u\\Vert \\Vert \\mathbf v \\Vert$,  and the decomposition looks like \n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "<img width=800px src=\"./images/svd_01.png\"></img>\n",
    "</center>   \n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "#### Example : SVD as sum of outer products\n",
    "\n",
    "The general SVD can be written as the sum of outer products  as\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\sum_{i=1}^r \\sigma_i \\mathbf u_i \\mathbf v_i^*\n",
    "\\end{equation}\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "<img width=900px src=\"./images/svd_02.png\"></img>\n",
    "</center>   \n",
    "\n",
    "This particular view of the SVD is what makes it a powerful tool.  If we have only a few singular values that are large, and the remaining are negligible, or zero, we can \"compress\" $A$ by representing it using only a few of the \n",
    "singular value/vectors.  \n",
    "\n",
    "\\begin{equation}\n",
    "A \\approx \\sum_{i=1}^s \\sigma_i \\mathbf u_i \\mathbf v_i^*, \\qquad s \\ll r\n",
    "\\end{equation}\n",
    "\n",
    "This is a key idea behind image compression.  Suppose the entres of $A$ represent pixel values in $[0,1]$ in a color channel (e.g. red, green or blue) in a $1024 \\times 1024$ image.   For each channel, the full image would require $1024^2$ bytes (or about 1MB of storage).  But if we can store the same image with only 10 singular vectors/values, we can get an image compression ratio of $(2 \\times 10 \\times 1024 + 10)/1024^2 \\approx 0.01$, or a compression rate of about 1\\% for each color channel.\n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "### Example : Connection to eigenvalues\n",
    "\n",
    "Suppose we have a diagonalizable matrix $A$.  Is there a connection between the eigenvalues/vectors and the singular values/vectors of $A$?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Given the SVD of a matrix $A$, we can write\n",
    "\n",
    "\\begin{equation}\n",
    "A^*A = (U \\Sigma V^*)^*(U \\Sigma V^*) = (V \\Sigma^* U^*)(U \\Sigma V^*) = V (\\Sigma^* \\Sigma)V^*\n",
    "\\end{equation}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{equation}\n",
    "AA^* = (U \\Sigma V^*)(U \\Sigma V^*)^* = (U \\Sigma V^*)(V \\Sigma^* U^*) = U (\\Sigma \\Sigma^*)U^*\n",
    "\\end{equation}\n",
    "\n",
    "The right singular vectors $V$ are the eigenvctors of $A^*A$ and the left singular vectors $U$ are the eigenvectors of $AA^*$.    The singular values are the square roots of the eigenvalues of $A^*A$ or $AA^*$.  \n",
    "\n",
    "If $A^*A = AA^*$, .e.g. $A$ is a normal matrix, then $A$ is unitarily diagonalizable as $A = QDQ^*$. If $A$ is also positive definite, than the eigenvalue decomposition and the singular value decomposition are the same.\n",
    "\n",
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "### Proof of the existence of the SVD\n",
    "\n",
    "See Theorem 4.1 (Lecture 4, TB, page 29).  The proof is by construction. \n",
    "\n",
    "* The singular values are unique\n",
    "\n",
    "* The singular vectors are unique up to complex constants of magnitude 1, e.g. $z = e^{i\\theta}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border-width:2px; border-color:black\"></hr>\n",
    "\n",
    "### Computation (by hand) of an SVD\n",
    "\n",
    "To compute the singular values/vectors of a matrix $A \\in R^{\\mathbb m \\times n}$ : \n",
    "\n",
    "1. Find the eigenvectors and eigenvalues of $A^*A$.  The eigenvectors are the right singular vectors $\\mathbf v$ of $A$, and the eigenvalues are the square of the singular values, e.g. $\\sigma^2 = \\lambda$.   \n",
    "\n",
    "2.  For $\\sigma \\ne 0$, compute left singular vectors $\\mathbf u$ from $\\mathbf u = A\\mathbf v/\\sigma$. \n",
    "\n",
    "The above will can be used to construct a \"reduced\" SVD.  To construct a full SVD, we would also need to complete $U$ and $V$ with additional vectors from the complement of the spaces spanned by the singular vectors found above. \n",
    "\n",
    "#### Example : Compute the SVD \n",
    "\n",
    "The Python function below computes a reduced SVD of an input matrix $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def SVD(A):\n",
    "\n",
    "    AtA = A.transpose()*A\n",
    "    S2 = AtA.eigenvects()  # Tuples : (eval, multiplicity, Matrix of evecs)\n",
    "    print(\"AtA.eigenvects() = \")\n",
    "    display(S2)\n",
    "    print(\"\")\n",
    "\n",
    "    m,n = A.shape\n",
    "\n",
    "    U = sp.Matrix(m,0,[])\n",
    "    V = sp.Matrix(n,0,[])\n",
    "    r = A.rank()  # Probably uses the SVD ...\n",
    "    S = sp.Matrix(r,r,[0]*r*r)\n",
    "    k = 0\n",
    "    for i,t in enumerate(S2):    \n",
    "        # display(t)\n",
    "        s = sp.sqrt(t[0])\n",
    "\n",
    "        # Only store non-zero singular values\n",
    "        if s > 0:            \n",
    "            S[k,k] = s\n",
    "            for v in t[2]:\n",
    "                v = v/v.norm(2)\n",
    "                V = sp.Matrix.hstack(V,v)\n",
    "\n",
    "                u = A*v/s\n",
    "                u = u/u.norm(2)\n",
    "                U = sp.Matrix.hstack(U,u)\n",
    "            k += 1\n",
    "    return U,S,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0\\\\0 & 0 & 0\\\\0 & 0 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0],\n",
       "[0, 0, 0],\n",
       "[0, 0, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = sp.Matrix(2,2,[2,2,-1,1])\n",
    "\n",
    "# Problem 4.1 (a)\n",
    "A = sp.Matrix(2,2,[3, 0, 0, -2])\n",
    "\n",
    "# Problem 4.1 (c)\n",
    "A = sp.Matrix(3,2,[0,2,0,0,0,0])\n",
    "\n",
    "# Problem 4.1 (e)\n",
    "A = sp.Matrix(4,3,[1]*12)\n",
    "\n",
    "# More examples\n",
    "u = sp.Matrix(3,1,[3]*3)\n",
    "v = sp.Matrix(2,1,[-1]*2)\n",
    "A = u*v.transpose()  # An outer product\n",
    "\n",
    "# Examples using symbolic values. \n",
    "a,b = sp.symbols('a b')\n",
    "A = sp.Matrix(3,3,[a,1,0,0,a,0,0,0,b])\n",
    "A = A.subs({a : 0, b : 2})\n",
    "\n",
    "\n",
    "display(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtA.eigenvects() = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [1],\n",
       "   [0],\n",
       "   [0]])]),\n",
       " (1,\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [0],\n",
       "   [1],\n",
       "   [0]])]),\n",
       " (4,\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [0],\n",
       "   [0],\n",
       "   [1]])])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "U = \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0\\\\0 & 0\\\\0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0],\n",
       "[0, 0],\n",
       "[0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sigma = \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0\\\\0 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0],\n",
       "[0, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V = \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0\\\\1 & 0\\\\0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0],\n",
       "[1, 0],\n",
       "[0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U*S*V^T = \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0\\\\0 & 0 & 0\\\\0 & 0 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0],\n",
       "[0, 0, 0],\n",
       "[0, 0, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U, S, V = SVD(A)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"U = \")\n",
    "display(U)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Sigma = \")\n",
    "display(S)\n",
    "print(\"\")\n",
    "\n",
    "print(\"V = \")\n",
    "display(V)\n",
    "print(\"\")\n",
    "print(\"U*S*V^T = \")\n",
    "display(U*S*V.transpose())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
